/* See COPYRIGHT for copyright information. */

#include <machine/x86.h>
#include <machine/pmap.h>
#include <machine/thread.h>
#include <machine/multiboot.h>
#include <dev/kclock.h>
#include <kern/lib.h>
#include <kern/kobj.h>
#include <inc/error.h>

static bool_t scrub_free_pages = 0;

// These variables are set by i386_detect_memory()
static physaddr_t maxpa;	// Maximum physical address
size_t npage;			// Amount of physical memory (in pages)
static size_t basemem;		// Amount of base memory (in bytes)
static size_t extmem;		// Amount of extended memory (in bytes)

// These variables are set in i386_vm_init()
static char *boot_freemem;	// Pointer to next byte of free mem
static struct Page_list page_free_list;
				// Free list of physical pages

// Global page allocation stats
struct page_stats page_stats;

static int
nvram_read (int r)
{
  return mc146818_read (NULL, r) | (mc146818_read (NULL, r + 1) << 8);
}

static void
i386_detect_memory (struct multiboot_info *mbi)
{
    if (mbi && (mbi->flags & MULTIBOOT_INFO_MEMORY)) {
	basemem = ROUNDDOWN(mbi->mem_lower * 1024, PGSIZE);
	extmem = ROUNDDOWN(mbi->mem_upper * 1024, PGSIZE);
    } else {
	// CMOS tells us how many kilobytes there are
	basemem = ROUNDDOWN (nvram_read (NVRAM_BASELO) * 1024, PGSIZE);
	extmem = ROUNDDOWN (nvram_read (NVRAM_EXTLO) * 1024, PGSIZE);
    }

    // Calculate the maxmium physical address based on whether
    // or not there is any extended memory.  See comment in ../inc/mmu.h.
    if (extmem)
	maxpa = EXTPHYSMEM + extmem;
    else
	maxpa = basemem;

    npage = maxpa / PGSIZE;

    cprintf("Physical memory: %dK available, ", (int) (maxpa / 1024));
    cprintf("base = %dK, extended = %dK\n", (int) (basemem / 1024),
	    (int) (extmem / 1024));
}

//
// Allocate n bytes of physical memory aligned on an 
// align-byte boundary.  Align must be a power of two.
// Return kernel virtual address.  Returned memory is uninitialized.
//
// If we're out of memory, boot_alloc should panic.
// It's too early to run out of memory.
// This function may ONLY be used during initialization,
// before the page_free_list has been set up.
// 
static void *
boot_alloc (uint32_t n, uint32_t align)
{
  extern char end[];
  void *v;

  // Initialize boot_freemem if this is the first time.
  // 'end' is a magic symbol automatically generated by the linker,
  // which points to the end of the kernel's bss segment -
  // i.e., the first virtual address that the linker
  // did _not_ assign to any kernel code or global variables.
  if (boot_freemem == 0)
    boot_freemem = end;

  boot_freemem = (char *) ROUNDUP (boot_freemem, align);
  if (boot_freemem + n < boot_freemem
      || boot_freemem + n > (char *) (maxpa + KERNBASE))
    panic ("out of memory during i386_vm_init");
  v = boot_freemem;
  boot_freemem += n;
  return v;
}

void
page_free (void *v)
{
    struct Page *p = (struct Page *) v;
    if (PGOFF(p))
	panic("page_free: not a page-aligned pointer %p", p);

    if (scrub_free_pages)
	memset(v, 0xde, PGSIZE);

    LIST_INSERT_HEAD (&page_free_list, p, pp_link);
    page_stats.pages_avail++;
    page_stats.pages_used--;
}

int
page_alloc (void **vp)
{
    struct Page *p = LIST_FIRST(&page_free_list);
    if (p) {
	LIST_REMOVE(p, pp_link);
	*vp = p;
	page_stats.pages_avail--;
	page_stats.pages_used++;
	page_stats.allocations++;
	return 0;
    }

    cprintf("page_alloc: returning no mem\n");
    page_stats.failures++;
    return -E_NO_MEM;
}

static void
page_init (void)
{
    int inuse;

    // Align boot_freemem to page boundary.
    boot_alloc (0, PGSIZE);

    for (uint64_t i = 0; i < npage; i++) {
	// Off-limits until proven otherwise.
	inuse = 1;

	// The bottom basemem bytes are free except page 0.
	if (i != 0 && i < basemem / PGSIZE)
	    inuse = 0;

	// The IO hole and the kernel abut.

	// The memory past the kernel is free.
	if (i >= RELOC (boot_freemem) / PGSIZE)
	    inuse = 0;

	if (!inuse)
	    page_free(pa2kva(i << PGSHIFT));
    }

    page_stats.pages_used = 0;
}

void
pmap_init (struct multiboot_info *mbi)
{
    i386_detect_memory (mbi);
    page_init ();
}

int
page_map_alloc (struct Pagemap **pm_store)
{
    void *pmap;
    int r = page_alloc(&pmap);
    if (r < 0)
	return r;

    memcpy(pmap, &bootpml4, PGSIZE);
    *pm_store = (struct Pagemap *) pmap;
    return 0;
}

static void
page_map_free_level (struct Pagemap *pgmap, int pmlevel)
{
    // Skip the kernel half of the address space
    int maxi = (pmlevel == 3 ? NPTENTRIES/2 : NPTENTRIES);
    int i;

    for (i = 0; i < maxi; i++) {
	uint64_t ptent = pgmap->pm_ent[i];
	if (!(ptent & PTE_P))
	    continue;
	if (pmlevel > 0) {
	    struct Pagemap *pm = (struct Pagemap *) pa2kva(PTE_ADDR(ptent));
	    page_map_free_level(pm, pmlevel - 1);
	}
    }

    page_free(pgmap);
}

void
page_map_free (struct Pagemap *pgmap)
{
    page_map_free_level (pgmap, 3);
}

//
// Stores address of page table entry in *ppte.
// Stores 0 if there is no such entry or on error.
// 
// RETURNS: 
//   0 on success
//   -E_NO_MEM, if page table couldn't be allocated
//
static int
pgdir_walk (struct Pagemap *pgmap, int pmlevel,
	    const void *va, int create, uint64_t **pte_store)
{
    assert(pmlevel >= 0 && pmlevel <= 3);

    uint64_t *pm_entp = &pgmap->pm_ent[PDX(pmlevel, va)];

    // If we made it all the way down, return the PTE
    if (pmlevel == 0) {
	*pte_store = pm_entp;
	return 0;
    }

    // We don't handle superpages (2MB) yet
    if ((*pm_entp & PTE_PS))
	return -E_INVAL;

    // If an intermediate page map is missing, allocate it (if asked for)
    if (!(*pm_entp & PTE_P)) {
	if (!create) {
	    *pte_store = 0;
	    return 0;
	}

	void *p;
	int r = page_alloc(&p);
	if (r < 0)
	    return r;

	memset(p, 0, PGSIZE);
	*pm_entp = kva2pa(p) | PTE_P | PTE_U | PTE_W;
    }

    struct Pagemap *pm_next = (struct Pagemap *) pa2kva(PTE_ADDR(*pm_entp));
    return pgdir_walk(pm_next, pmlevel-1, va, create, pte_store);
}

//
// Return the page mapped at virtual address 'va'.
// If pte_store is not zero, then we store in it the address
// of the pte for this page.  This is used by page_remove
// but should not be used by other callers.
//
// Return 0 if there is no page mapped at va.
//
static void *
page_lookup_internal (struct Pagemap *pgmap, void *va, uint64_t **pte_store)
{
    if ((uintptr_t) va >= ULIM)
	panic("page_lookup_internal: va %p over ULIM", va);

    uint64_t *ptep;
    int r = pgdir_walk(pgmap, 3, va, 0, &ptep);
    if (r < 0)
	panic("pgdir_walk(%p, create=0) failed: %d", va, r);

    if (pte_store)
	*pte_store = ptep;

    if (ptep == 0 || !(*ptep & PTE_P))
	return 0;

    return pa2kva (PTE_ADDR (*ptep));
}

void *
page_lookup (struct Pagemap *pgmap, void *va)
{
    if ((uintptr_t) va >= ULIM)
	return 0;

    return page_lookup_internal(pgmap, va, 0);
}

//
// Invalidate a TLB entry, but only if the page tables being
// edited are the ones currently in use by the processor.
//
static void
tlb_invalidate (struct Pagemap *pgmap, void *va)
{
    // Flush the entry only if we're modifying the current address space.
    if (cur_thread == 0)
	return;
    if (cur_thread->th_as == 0)
	return;
    if (cur_thread->th_as->as_pgmap == pgmap)
	invlpg(va);
}

//
// Unmaps the physical page at virtual address 'va'.
//
// Details:
//   - The pg table entry corresponding to 'va' should be set to 0.
//     (if such a PTE exists)
//   - The TLB must be invalidated if you remove an entry from
//     the pg dir/pg table.
//
void
page_remove (struct Pagemap *pgmap, void *va)
{
    uint64_t *ptep;
    void *p = page_lookup_internal(pgmap, va, &ptep);
    if (p == 0)
	return;

    *ptep = 0;
    tlb_invalidate(pgmap, va);
}

//
// Map the physical page 'pp' at virtual address 'va'.
// The permissions (the low 12 bits) of the page table
//  entry should be set to 'perm|PTE_P'.
//
// Details
//   - If there is already a page mapped at 'va', returns -E_BUSY.
//   - If necesary, allocates a page table and inserts it into 'pgdir'.
//
// RETURNS: 
//   0 on success
//   -E_NO_MEM, if page table couldn't be allocated
//   -E_BUSY, if another page is already mapped at va
//
int
page_insert (struct Pagemap *pgmap, void *page, void *va, uint64_t perm)
{
    uint64_t *ptep;
    int r = pgdir_walk(pgmap, 3, va, 1, &ptep);
    if (r < 0)
	return r;

    if (*ptep & PTE_P)
	return -E_BUSY;

    *ptep = kva2pa(page) | perm | PTE_P;
    return 0;
}

int
page_user_incore(void **ptrp, uint64_t nbytes)
{
    uintptr_t ptr = (uintptr_t) *ptrp;
    ptr &= ~(1L << 63);

    if (cur_thread == 0 || cur_thread->th_as == 0)
	panic("page_user_incore: no thread or address space");
    const struct Address_space *as = cur_thread->th_as;

    // XXX this might not deal so well with writable pages that are mapped RO
    // for snapshotting....

    if (nbytes > 0) {
	uintptr_t end = ROUNDUP(ptr + nbytes, PGSIZE);
	for (uintptr_t va = ROUNDDOWN(ptr, PGSIZE); va < end; va += PGSIZE) {
	    if (page_lookup(as->as_pgmap, (void*) va) == 0) {
		int r = as_pagefault(&kobject_dirty(&as->as_ko)->u.as, (void*) va);
		if (r < 0)
		    return r;
	    }
	}
    }

    *ptrp = (void*) ptr;
    return 0;
}
